{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7792d9-9e25-4ca9-9e56-b3c7d5fb2339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ad9a0c0-ec5e-41d3-8d08-52d4c8354392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dados de acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a1ee713-9a80-48a6-9874-3bda935b76e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verificar as keyvaults\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"05385f3d-aff7-4b2a-a354-e55490cf3c3c\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'WBx8Q~_K9~ON-VTsZLq_xMdMsFF6BZGSumDEQcoF',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cc1ec62c-0411-4afb-a12a-a1b2e028414a/oauth2/token\"}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://bronze@dlsinbevtest.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/inbev/\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf1e7c0d-1863-4ce7-9c84-a2a57bb4ae04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Mount points - 1 mount por pasta pré-criada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a3b264-2c2b-447d-9161-3b3779bfd869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verificar as keyvaults\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"05385f3d-aff7-4b2a-a354-e55490cf3c3c\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'WBx8Q~_K9~ON-VTsZLq_xMdMsFF6BZGSumDEQcoF',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cc1ec62c-0411-4afb-a12a-a1b2e028414a/oauth2/token\"}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://silver@dlsinbevtest.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/silver/\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e5c82b-f194-4e86-8e00-d793995d8ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verificar as keyvaults\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"05385f3d-aff7-4b2a-a354-e55490cf3c3c\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'WBx8Q~_K9~ON-VTsZLq_xMdMsFF6BZGSumDEQcoF',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cc1ec62c-0411-4afb-a12a-a1b2e028414a/oauth2/token\"}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://silver@dlsinbevtest.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/silver/\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81bb4622-6c74-4820-be51-d956ed08e092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verificar as keyvaults\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"05385f3d-aff7-4b2a-a354-e55490cf3c3c\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'WBx8Q~_K9~ON-VTsZLq_xMdMsFF6BZGSumDEQcoF',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cc1ec62c-0411-4afb-a12a-a1b2e028414a/oauth2/token\"}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"abfss://gold@dlsinbevtest.dfs.core.windows.net\", # contrainer@storageacc\n",
    "mount_point = \"/mnt/inbev/gold\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1091f6-33e5-4f26-b217-5c5a0eb8a48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Leitura dos dados da bronze source, contrato de dados e elaboração da camada prata com salvamento em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6f5733-8454-4bd7-a70c-c8e161582693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "\n",
    "# Contrato de dados\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), nullable=False),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"brewery_type\", StringType()),\n",
    "    StructField(\"address_1\", StringType()),\n",
    "    StructField(\"address_2\", StringType()),\n",
    "    StructField(\"address_3\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"state_province\", StringType()),\n",
    "    StructField(\"postal_code\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"longitude\", StringType()),\n",
    "    StructField(\"latitude\", StringType()),\n",
    "    StructField(\"phone\", StringType()),\n",
    "    StructField(\"website_url\", StringType()),\n",
    "    StructField(\"state\", StringType()),\n",
    "    StructField(\"street\", StringType())\n",
    "])\n",
    "\n",
    "# Caminho para o arquivo JSON no mount point\n",
    "json_file_path = \"/mnt/inbev/data_a041638e-b655-48ff-962d-004c5f15d048_a566ec44-c3db-497c-94cd-4d13888e7d60.json\"  \n",
    "\n",
    "# Lê o arquivo JSON diretamente para um DataFrame, aplicando o schema\n",
    "try:\n",
    "    df_bronze = spark.read.json(json_file_path, schema=schema)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo JSON: {e}\")\n",
    "    # Trate o erro de acordo com sua necessidade. Por exemplo:\n",
    "    # raise  # Re-lança a exceção para interromper o processamento\n",
    "    # df_bronze = spark.createDataFrame([], schema=schema) # Cria um DataFrame vazio com o schema para continuar o código.\n",
    "\n",
    "def create_silver_layer(df_bronze):\n",
    "    \"\"\"Cria a camada prateada.\"\"\"\n",
    "    df_prata = df_bronze.withColumn(\"longitude\", col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", col(\"latitude\").cast(\"double\"))\n",
    "    return df_prata\n",
    "\n",
    "# Cria a camada prata\n",
    "df_prata = create_silver_layer(df_bronze)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet (adapte o caminho conforme necessário)\n",
    "parquet_file_path = \"/mnt/inbev/silver/breweries.parquet\" # Exemplo dentro do mesmo mount\n",
    "df_prata.write.parquet(parquet_file_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Arquivo Parquet salvo em: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd95afe-e223-4ad0-9f71-8592a44dd8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leitura do arquivo da prata e agregações pra gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a095c5a-e525-4d0a-a46d-6b1e7047b6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "def create_brewery_aggregation(df_prata):\n",
    "    \"\"\"\n",
    "    Cria uma visão agregada com a quantidade de cervejarias por tipo e localização.\n",
    "\n",
    "    Args:\n",
    "        df_prata: DataFrame da camada prata contendo os dados das cervejarias.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame Spark contendo a agregação.\n",
    "    \"\"\"\n",
    "\n",
    "    df_agg = df_prata.groupBy(\"brewery_type\", \"state\", \"city\").agg(count(\"*\").alias(\"quantidade_cervejarias\"))\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# ... (código anterior para ler e processar os dados)\n",
    "\n",
    "# Cria a visão agregada\n",
    "df_agregado = create_brewery_aggregation(df_prata)\n",
    "\n",
    "# Exibe o resultado\n",
    "df_agregado.show()\n",
    "\n",
    "\n",
    "# Salva o DataFrame em formato Parquet (adapte o caminho conforme necessário)\n",
    "parquet_file_path = \"/mnt/inbev/gold/breweries.parquet\" # Exemplo dentro do mesmo mount\n",
    "df_agregado.write.parquet(parquet_file_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Arquivo Parquet salvo em: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b439c45-38f2-4632-9d04-72d2edee6c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "def create_brewery_aggregation(df_prata):\n",
    "    \"\"\"\n",
    "    Cria uma visão agregada com a quantidade de cervejarias por tipo e localização.\n",
    "\n",
    "    Args:\n",
    "        df_prata: DataFrame da camada prata contendo os dados das cervejarias.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame Spark contendo a agregação.\n",
    "    \"\"\"\n",
    "\n",
    "    df_agg = df_prata.groupBy(\"brewery_type\", \"state\", \"city\").agg(count(\"*\").alias(\"quantidade_cervejarias\"))\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# ... (código anterior para ler e processar os dados)\n",
    "\n",
    "# Cria a visão agregada\n",
    "df_agregado = create_brewery_aggregation(df_prata)\n",
    "\n",
    "# Exibe o resultado\n",
    "df_agregado.show()\n",
    "\n",
    "\n",
    "# Salva o DataFrame em formato Parquet (adapte o caminho conforme necessário)\n",
    "parquet_file_path = \"/mnt/inbev/gold/breweries.parquet\" # Exemplo dentro do mesmo mount\n",
    "df_agregado.write.parquet(parquet_file_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Arquivo Parquet salvo em: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4e23f2-01fd-402e-84ba-838e767e27bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Persistência e Disponibilização dos dados da Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d9e37f-c3a5-4091-8399-f2881762fb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# encapsulamento de dados sensíveis - usuário e senha\n",
    "\n",
    "user = 'serveradmin'\n",
    "password = \"#InbevServer\"\n",
    "# COMMAND ----------\n",
    "\n",
    "# carga na tabela SQL\n",
    "\n",
    "def sql_spark(df):\n",
    "    df.write.format('jdbc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('url', \"jdbc:sqlserver://sql-serv-inbevtest.database.windows.net\")\\\n",
    "    .option(\"databaseName\", \"sqldb.inbevtest\")\\\n",
    "    .option('dbtable', 'Breweries')\\\n",
    "    .option('password', password)\\\n",
    "    .option('user', user)\\\n",
    "    .save()\n",
    "\n",
    "sql_spark(df_agregado)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9d6ec5-b692-4230-88e0-98f4eb13b6d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''# encapsulamento de dados sensíveis - usuário e senha\n",
    "\n",
    "user = dbutils.secrets.get(scope = \"thatsbluefolks-db\", key = \"db-user\")\n",
    "password = dbutils.secrets.get(scope = \"thatsbluefolks-db\", key = \"db-password\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# carga na tabela SQL\n",
    "\n",
    "def sql_spark(dados):\n",
    "    df_agregado.write.format('jdbc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('url', \"jdbc:sqlserver://srv-thatsbluefolks.database.windows.net\")\\\n",
    "    .option(\"databaseName\", \"db-thatsbluefolks\")\\\n",
    "    .option('dbtable', 'EducacaoFinanceira_App.CotacaoDiaria')\\\n",
    "    .option('password', password)\\\n",
    "    .option('user', user)\\\n",
    "    .save()\n",
    "sql_spark(df)'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL Inbev",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
