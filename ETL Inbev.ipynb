{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7792d9-9e25-4ca9-9e56-b3c7d5fb2339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad9a0c0-ec5e-41d3-8d08-52d4c8354392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Dados de acesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0329cd3b-2f3e-4178-8493-924314c5fd31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": \"05385f3d-aff7-4b2a-a354-e55490cf3c3c\",\n",
    "\"fs.azure.account.oauth2.client.secret\": 'WBx8Q~_K9~ON-VTsZLq_xMdMsFF6BZGSumDEQcoF',\n",
    "\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cc1ec62c-0411-4afb-a12a-a1b2e028414a/oauth2/token\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb65bb4-4db2-49c9-a0d7-3973a3c00708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lake = dbutils.secrets.get(scope = \"scopedbwinbevtest\", key = \"secret-datalake-name\")\n",
    "lake_key = dbutils.secrets.get(scope = \"scopedbwinbevtest\", key = \"secret-key-datalake\")\n",
    "containers = ['bronze', 'silver', 'gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce8487d-c630-40bb-b0ce-1f0854a2caa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(lake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e212ffa1-189f-48cb-bb7d-536677547a30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mount_datalake(container):\n",
    "    try:\n",
    "        for container in containers:  # Fixed typo from 'contanier' to 'container'\n",
    "            dbutils.fs.mount(\n",
    "                source = f\"wabfss://{container}@{lake}.dfs.core.windows.net\",\n",
    "                mount_point = f\"/mnt/{container}\",  # Fixed mount_point to use 'container'\n",
    "                extra_configs = configs\n",
    "            )\n",
    "    except ValueError as e:  # Fixed exception type from 'valueError' to 'ValueError'\n",
    "        print(f\"Erro ao montar o datalake: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876edbab-4d6b-47c9-aba0-3e0da31e3686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Caminho para o arquivo JSON no mount point\n",
    "\n",
    "json_file_path = \"/mnt/bronze/\"\n",
    "\n",
    "def extract_data():\n",
    "    url = 'https://api.openbrewerydb.org/breweries'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Salva o JSON no bucket bronze\n",
    "    with open(\"/dbfs\" + json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a1091f6-33e5-4f26-b217-5c5a0eb8a48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Leitura dos dados da bronze source, contrato de dados e elaboração da camada prata com salvamento em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6f5733-8454-4bd7-a70c-c8e161582693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "\n",
    "# Contrato de dados\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), nullable=False),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"brewery_type\", StringType()),\n",
    "    StructField(\"address_1\", StringType()),\n",
    "    StructField(\"address_2\", StringType()),\n",
    "    StructField(\"address_3\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"state_province\", StringType()),\n",
    "    StructField(\"postal_code\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"longitude\", StringType()),\n",
    "    StructField(\"latitude\", StringType()),\n",
    "    StructField(\"phone\", StringType()),\n",
    "    StructField(\"website_url\", StringType()),\n",
    "    StructField(\"state\", StringType()),\n",
    "    StructField(\"street\", StringType())\n",
    "])\n",
    "\n",
    "json_file_path = \"/mnt/bronze/data_a041638e-b655-48ff-962d-004c5f15d048_a566ec44-c3db-497c-94cd-4d13888e7d60.json\" \n",
    "# Lê o arquivo JSON diretamente para um DataFrame, aplicando o schema\n",
    "try:\n",
    "    df_bronze = spark.read.json(json_file_path, schema=schema)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo JSON: {e}\")\n",
    "    # Trate o erro de acordo com sua necessidade. Por exemplo:\n",
    "    # raise  # Re-lança a exceção para interromper o processamento\n",
    "    df_bronze = spark.createDataFrame([], schema=schema)\n",
    "\n",
    "def create_silver_layer(df_bronze):\n",
    "    \"\"\"Cria a camada prata.\"\"\"\n",
    "    df_prata = df_bronze.withColumn(\"longitude\", col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", col(\"latitude\").cast(\"double\"))\n",
    "    return df_prata\n",
    "\n",
    "# Cria a camada prata\n",
    "df_prata = create_silver_layer(df_bronze)\n",
    "\n",
    "# Salva o DataFrame em formato Parquet (adapte o caminho conforme necessário)\n",
    "parquet_file_path = \"/mnt/silver/breweries.parquet\" # Exemplo dentro do mesmo mount\n",
    "df_prata.write.parquet(parquet_file_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Arquivo Parquet salvo em: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd95afe-e223-4ad0-9f71-8592a44dd8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leitura do arquivo da prata e agregações pra gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a095c5a-e525-4d0a-a46d-6b1e7047b6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "def create_brewery_aggregation(df_prata):\n",
    "    \"\"\"\n",
    "    Cria uma visão agregada com a quantidade de cervejarias por tipo e localização.\n",
    "\n",
    "    Args:\n",
    "        df_prata: DataFrame da camada prata contendo os dados das cervejarias.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame Spark contendo a agregação.\n",
    "    \"\"\"\n",
    "\n",
    "    df_agg = df_prata.groupBy(\"brewery_type\", \"state\", \"city\").agg(count(\"*\").alias(\"quantidade_cervejarias\"))\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "# Cria a visão agregada\n",
    "df_agregado = create_brewery_aggregation(df_prata)\n",
    "\n",
    "# Exibe o resultado\n",
    "#df_agregado.show()\n",
    "\n",
    "\n",
    "# Salva o DataFrame em formato Parquet (adapte o caminho conforme necessário)\n",
    "parquet_file_path = \"/mnt/gold/breweries.parquet\" # Exemplo dentro do mesmo mount\n",
    "df_agregado.write.parquet(parquet_file_path, mode=\"overwrite\")\n",
    "\n",
    "print(f\"Arquivo Parquet salvo em: {parquet_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4e23f2-01fd-402e-84ba-838e767e27bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Persistência e Disponibilização dos dados da Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d9e37f-c3a5-4091-8399-f2881762fb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user = dbutils.secrets.get(scope = \"scopedbwinbevtest\", key = \"secret-sql-user\")\n",
    "password = dbutils.secrets.get(scope = \"scopedbwinbevtest\", key = \"secret-sql-pass\")\n",
    "\n",
    "# carga na tabela SQL\n",
    "\n",
    "def sql_spark(df):\n",
    "    df.write.format('jdbc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .option('url', \"jdbc:sqlserver://sql-serv-inbevtest.database.windows.net\")\\\n",
    "    .option(\"databaseName\", \"sqldb.inbevtest\")\\\n",
    "    .option('dbtable', 'Breweries_gold')\\\n",
    "    .option('password', password)\\\n",
    "    .option('user', user)\\\n",
    "    .save()\n",
    "\n",
    "sql_spark(df_agregado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d949a50-2480-4541-bfea-c135954d4ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Desmonta as camAdas DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc053783-0ae9-44fb-8e09-b58625dfaa2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def unmount_datalake(containers):\n",
    "    try:\n",
    "        for container in containers:\n",
    "            dbutils.fs.unmount(f\"/mnt/{container}/\")\n",
    "    except ValueError as err:\n",
    "        print(f\"Erro ao desmontar o datalake: {err}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5524098496929777,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL Inbev",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
